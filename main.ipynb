{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOedAej/x/hgw503As1nDSY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bharatgirdhar/DeepVisionProject/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "P1T8qlDz7XUj",
        "outputId": "067ed54b-d790-4249-b0c6-6a46780cf072"
      },
      "source": [
        "'''Train CIFAR10 with PyTorch.'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#from models import *\n",
        "#from utils import progress_bar\n",
        "\n",
        "\n",
        "#parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
        "#parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
        "#parser.add_argument('--resume', '-r', action='store_true',\n",
        "#                    help='resume from checkpoint')\n",
        "#args = parser.parse_args()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "\n",
        "#tMean = (0.4914, 0.4822, 0.4465)\n",
        "#tStandardDev = (0.2023, 0.1994, 0.2010)\n",
        "#ApplyTransformations(vMean,tStandardDev)\n",
        "\n",
        "\n",
        "\n",
        "def CreateCIFAR10TrainSet(vBatchSize, transform_train):\n",
        "  trainset = torchvision.datasets.CIFAR10(\n",
        "      root='./data', train=True, download=True, transform=transform_train)\n",
        "  trainloader = torch.utils.data.DataLoader(\n",
        "      trainset, batch_size=vBatchSize, shuffle=True, num_workers=2)\n",
        "  return trainloader\n",
        "def CreateCIFAR10TestSet(vBatchSize, transform_test):\n",
        "  testset = torchvision.datasets.CIFAR10(\n",
        "      root='./data', train=False, download=True, transform=transform_test)\n",
        "  testloader = torch.utils.data.DataLoader(\n",
        "      testset, batch_size=vBatchSize, shuffle=False, num_workers=2)\n",
        "  return testloader\n",
        "#classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "#           'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Model\n",
        "#print('==> Building model..')\n",
        "\n",
        "  # net = VGG('VGG19')\n",
        "#  net = ResNet18()\n",
        "  # net = PreActResNet18()\n",
        "  # net = GoogLeNet()\n",
        "  # net = DenseNet121()\n",
        "  # net = ResNeXt29_2x64d()\n",
        "  # net = MobileNet()\n",
        "  # net = MobileNetV2()\n",
        "  # net = DPN92()\n",
        "  # net = ShuffleNetG2()\n",
        "  # net = SENet18()\n",
        "  # net = ShuffleNetV2(1)\n",
        "  # net = EfficientNetB0()\n",
        "  # net = RegNetX_200MF()\n",
        "  #net = SimpleDLA()\n",
        "#  net = net.to(device)\n",
        "#  if device == 'cuda':\n",
        "#      net = torch.nn.DataParallel(net)\n",
        "#      cudnn.benchmark = True\n",
        "\n",
        "#if args.resume:\n",
        "    # Load checkpoint.\n",
        "#    print('==> Resuming from checkpoint..')\n",
        "#    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
        "#    checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
        "#    net.load_state_dict(checkpoint['net'])\n",
        "#    best_acc = checkpoint['acc']\n",
        "#    start_epoch = checkpoint['epoch']\n",
        "\n",
        "def SetOptimizer(vOptimizerType, vlr, net):\n",
        "  if vOptimizerType==1:\n",
        "    optimizer = optim.SGD(net.parameters(), lr=vlr,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "  return optimizer\n",
        "def SetScheduler(optimizer):\n",
        "  #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "  #scheduler = optim.lr_scheduler.OneCycleLR(optimizer, 0.19,,,512 lr_range=(0.1, 1.)\n",
        "  return scheduler\n",
        "\n",
        "def SetLoss(vLossType):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  return criterion\n",
        "\n",
        "test_losses=[]\n",
        "train_losses=[]\n",
        "\n",
        "test_accuracies=[]\n",
        "train_accuracies=[]\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch, vNormalizationType, criterion):\n",
        "    model.train()\n",
        "    train_loss=0\n",
        "    pbar = tqdm(train_loader, leave=\"false\")\n",
        "    correct=0\n",
        "    processed=0\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data['image'].float().to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        #pred = output.argmax(dim=1).float()\n",
        "        #print(output.requires_grad)\n",
        "        #pred.requires_grad=True\n",
        "        #print(pred.size())\n",
        "        #if vNormalizationType in (0,1):          \n",
        "        loss = criterion(output, target)\n",
        "        #else:          \n",
        "        #  loss = F.l1_loss(pred,target.float())\n",
        "          #print(pred.size())\n",
        "          #print('target',target.size())\n",
        "          #loss.requres_grad= False\n",
        "          #vL1=0\n",
        "          #for p in Net.Net(vNormalizationType).parameters():\n",
        "          #  vL1 += p.abs().sum()\n",
        "          #loss= loss + vL1Factor*vL1        \n",
        "        #print('loss',loss)\n",
        "        pred = output.argmax(dim=1)\n",
        "        train_loss += loss\n",
        "        loss.backward()        \n",
        "        optimizer.step()\n",
        "        \n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        processed += len(data)\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx} Accuracy={100.*correct/processed:0.2f}')\n",
        "    \n",
        "    train_losses.append(train_loss/len(train_loader.dataset))\n",
        "    train_accuracies.append(100.*correct/processed)\n",
        "    \n",
        "    print('\\n\\nEPOCH : ',epoch)\n",
        "    print(f\"\\nTrain Loss{train_loss/len(train_loader.dataset)}\")\n",
        "\n",
        "\n",
        "def test(model, device, test_loader,epoch,vNormalizationType):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    missed_pred=[]\n",
        "    j=0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(test_loader):\n",
        "            data_copy=data\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            #print(batch_idx)\n",
        "            if epoch==1 and batch_idx==99:\n",
        "              #print('Hi')\n",
        "              for i in range(len(target)):\n",
        "                #print('Hello')\n",
        "                if pred[i]!=target[i]:\n",
        "                  plt.imshow(data_copy[i].permute(1,2,0))\n",
        "                  plt.show()\n",
        "                  #break\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(100. * correct / len(test_loader.dataset))\n",
        "    \n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "\n",
        "# Training\n",
        "#def train(epoch):\n",
        "#    print('\\nEpoch: %d' % epoch)\n",
        "#    net.train()\n",
        "#    train_loss = 0\n",
        "#    correct = 0\n",
        "#    total = 0\n",
        "#    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "#        inputs, targets = inputs.to(device), targets.to(device)\n",
        "#        optimizer.zero_grad()\n",
        "#        outputs = net(inputs)\n",
        "#        loss = criterion(outputs, targets)\n",
        "#        loss.backward()\n",
        "#        optimizer.step()\n",
        "\n",
        "#        train_loss += loss.item()\n",
        "#        _, predicted = outputs.max(1)\n",
        "#        total += targets.size(0)\n",
        "#        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "#        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "#                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "\n",
        "#def test(epoch):\n",
        "#    global best_acc\n",
        "#    net.eval()\n",
        "#    test_loss = 0\n",
        "#    correct = 0\n",
        "#    total = 0\n",
        "#    with torch.no_grad():\n",
        "#        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "#            inputs, targets = inputs.to(device), targets.to(device)\n",
        "#            outputs = net(inputs)\n",
        "#            loss = criterion(outputs, targets)\n",
        "\n",
        "#            test_loss += loss.item()\n",
        "#            _, predicted = outputs.max(1)\n",
        "#            total += targets.size(0)\n",
        "#            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "#            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "#                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "    # Save checkpoint.\n",
        "#    acc = 100.*correct/total\n",
        "#    if acc > best_acc:\n",
        "#        print('Saving..')\n",
        "#        state = {\n",
        "#            'net': net.state_dict(),\n",
        "#            'acc': acc,\n",
        "#            'epoch': epoch,\n",
        "#        }\n",
        "#        if not os.path.isdir('checkpoint'):\n",
        "#            os.mkdir('checkpoint')\n",
        "#        torch.save(state, './checkpoint/ckpt.pth')\n",
        "#        best_acc = acc\n",
        "\n",
        "\n",
        "#for epoch in range(start_epoch, start_epoch+200):\n",
        "#    train(epoch)\n",
        "#    test(epoch)\n",
        "#    scheduler.step()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-9b3ca83204e2>\"\u001b[0;36m, line \u001b[0;32m95\u001b[0m\n\u001b[0;31m    return scheduler\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niVM-dYiD5N5"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60YUM5_oEBUW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae4vJzCzEjQF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}